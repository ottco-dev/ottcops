<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Debug Mode · OPENCORE Analyzer</title>
    <style>
      body {
        font-family: "Space Grotesk", system-ui, sans-serif;
        margin: 0 auto;
        max-width: 960px;
        padding: 2.5rem;
        line-height: 1.6;
        color: #0f172a;
      }
      header {
        border-bottom: 2px solid #0f172a;
        margin-bottom: 2rem;
      }
      pre {
        background: #0f172a;
        color: #f9fafc;
        padding: 1rem;
        border-radius: 8px;
        overflow-x: auto;
      }
      .doc-nav a {
        margin-right: 1rem;
        color: #0d5c63;
        text-decoration: none;
        font-weight: 600;
      }
    </style>
  </head>
  <body>
    <header>
      <h1>Debug Mode</h1>
      <p>Show latency, request IDs, and payload snippets for power users.</p>
      <nav class="doc-nav">
        <a href="/doc/index.html">Index</a>
        <a href="/doc/prompts.html">Prompts</a>
        <a href="/doc/batch.html">Batch</a>
        <a href="/doc/debug.html">Debug</a>
        <a href="/doc/api_token_mode.html">API Tokens</a>
        <a href="/doc/ui.html">UI</a>
        <a href="/doc/export.html">Export</a>
        <a href="/doc/streams.html">Streams</a>
        <a href="/doc/models.html">Models</a>
        <a href="/doc/modules.html">Add-ons</a>
        <a href="/doc/raspberry.html">Raspberry Pi</a>
        <a href="/doc/home_automation.html">Home Automation</a>
      </nav>
    </header>

    <section>
      <h2>Enabling</h2>
      <ul>
        <li>Toggle “Show debug” in the UI, or</li>
        <li>Add <code>?debug=1</code> to the URL.</li>
      </ul>
    </section>

    <section>
      <h2>What you see</h2>
      <ul>
        <li><strong>Request ID:</strong> Trace a specific run across logs.</li>
        <li><strong>Model version:</strong> Selected TM bundle and LLM profile.</li>
        <li><strong>Timings:</strong> Upload, ML inference, LLM call, and total.</li>
        <li><strong>Payload preview:</strong> Shortened request/response JSON.</li>
        <li><strong>Errors:</strong> Any exception text surfaced inline.</li>
      </ul>
    </section>

    <section>
      <h2>Interpreting timings</h2>
      <ul>
        <li><strong>High model_ms:</strong> Consider a lighter TM bundle or GPU.</li>
        <li><strong>High llm_ms:</strong> Check API token mode latency or provider load.</li>
        <li><strong>High total_ms:</strong> Network + model + LLM combined; stream sources can add delay.</li>
      </ul>
    </section>
  </body>
</html>
