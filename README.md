# OTTCOUTURE Cannabis Vision OpenCore

OTTCOPS ist der von [ottcouture.eu](https://ottcouture.eu) betriebene Analyzer fÃ¼r Cannabis-Vision. Er kombiniert Teachable-Machine-Modelle mit multimodalen LLMs und liefert strukturierte JSON-Outputs â€“ sachlich, reproduzierbar und vollstÃ¤ndig unter OTTCOUTURE-Rechten. Feedback oder neue Modelle gern an **otcdmin@outlook.com**, Instagram **@ottcouture.eu** oder Discord [`discord.gg/GMMSqePfPh`](https://discord.gg/GMMSqePfPh).

## Feature Highlights
- ğŸŒ¿ **FastAPI Core** mit Analyzer, Config Deck, OTTO-Chat (`/completions`) und dokumentierten `/tm-models*` Routen.
- ğŸ§  **Vision LLM Switchboard** fÃ¼r OpenAI, Ollama oder LM Studio inkl. System-Presetverwaltung, Mehrfach-Profilen und serverseitiger Persistenz fÃ¼r Analyzer, Streams und OTTO.
- ğŸ§ª **Teachable-Machine-Depot** mit ZIP-Uploads (TFJS: metadata.json/model.json/weights.bin oder Keras: keras_model.h5 + labels.txt), Registry und Standardauswahl fÃ¼r den Analyzer.
- ğŸ§µ **Model Routing**: Das Frontend kann pro Analyse den gewÃ¼nschten TM-Slot wÃ¤hlen; die Einstellung wird zusÃ¤tzlich serverseitig in `app-settings.json` persistiert.
- ğŸ¤– **OTTO Grow Chat** â€“ eigener Screen fÃ¼r kultivierungsrelevante Fragen mit definiertem System Prompt.
- ğŸ“¡ **WiFi Broadcast Mode** (mDNS/zeroconf) fÃ¼r Hostnamen wie `ottcolab.local` im gesamten WLAN.
- ğŸ“ **Prompt-Templates** inkl. lokaler Custom-Presets direkt im Analyzer.
- ğŸ—‚ï¸ **Batch-Analyse** mit `/api/opencore/analyze-batch`, Tabs pro Bild und Gesamt-Report.
- ğŸ› ï¸ **Debug-Panel** mit Request-ID, Modellversion und Timings (UI-Toggle + `?debug=1`).
- ğŸ” **API-Token-Mode**: Eigene Base-URL + Token, inkl. Code-Beispielen.
- ğŸ“¤ **Export-Paket**: JSON-Download, PDF-Report sowie Share-Links Ã¼ber `/api/opencore/share` + Viewer (`/share/<id>`).
- ğŸ§· **ML-only Analysemodus**: `analysis_mode=ml` liefert reine Teachable-Machine-JSONs ohne GPT-Laufzeit.
- ğŸ¥ **Stream-Orchestrierung**: Snapshot/RTSP-Quellen laufen als Hintergrundjobs (5â€¯s Capture, 30â€¯s Batch) und liefern automatische Reports.
- ğŸ”„ **Launch-Update-Check**: Bei jedem Start prÃ¼ft das Backend gegen `github.com/methoxy000/ottcops` und bietet ein optionales `git pull` an.

## Nutzung & Lizenzpflicht
- Der OPENCORE Analyzer darf ohne weitere Freigabe ausschlieÃŸlich von privaten Einzelnutzer:innen und Developer:innen zu Test- und Forschungszwecken betrieben werden.
- Cannabis Social Clubs (CSCs) und Unternehmen â€“ egal ob Start-up, MSO oder Dienstleister â€“ mÃ¼ssen vor Einsatz in kommerziellen Projekten direkt mit **ottcouture.eu** eine Lizenz vereinbaren.
- Kontakt fÃ¼r Lizenzen & Partnerschaften: **otcdmin@outlook.com**, Instagram **@ottcouture.eu**, Discord [`discord.gg/GMMSqePfPh`](https://discord.gg/GMMSqePfPh).

## Installation im OTTCOUTURE Style
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt

# optional wenn du GPT Calls willst
export OPENAI_API_KEY="sk-..."

# Dev-Server starten
uvicorn app:app --reload
```

Beim Start fÃ¼hrt der Server automatisch einen Git-Vergleich gegen `https://github.com/methoxy000/ottcops`. Wird ein neuer Commit gefunden, erscheint eine Konsolenabfrage (â€Jetzt aktualisieren?â€œ). Die Eingabe `y` oder `yes` startet ein `git pull`, jede andere Antwort lÃ¤sst die vorhandene Version aktiv. Setze `OTTC_SKIP_UPDATE_CHECK=1`, wenn der Check z.â€¯B. in CI-Pipelines Ã¼bersprungen werden soll.

1. Analyzer UI: `http://localhost:8000/`
2. OTTO Grow Chat: `http://localhost:8000/completions`
3. Config Hub inkl. TM-Depot: `http://localhost:8000/config`
4. Discord Crew & Support: [`discord.gg/GMMSqePfPh`](https://discord.gg/GMMSqePfPh)
5. Dokumentation (HTML): `http://localhost:8000/doc/index.html`

## Konfiguration
| Variable | Pflicht | Default | Beschreibung |
| --- | --- | --- | --- |
| `OPENAI_API_KEY` | bei OpenAI Flow | â€“ | Key fÃ¼r GPT-4.1 mini oder dein bevorzugtes Vision Modell. |
| `OPENAI_GPT_MODEL` | optional | `gpt-4.1-mini` | LLM-ID fÃ¼r Cloud Vision. |
| `TEACHABLE_MODEL_PATH` | optional | `./models/teachable_model` | Alternativer Pfad zu einem Legacy-Teachable-Model. |

Die Provider-/LLM-Konfiguration aus dem Config Hub wird lokal (`localStorage.cannabisLLMConfig`) und serverseitig via `/api/settings/llm` gespeichert. Mehrere Profile lassen sich Ã¼ber `/api/settings/llm/profiles` anlegen, aktivieren oder lÃ¶schen; die Auswahl erscheint im Analyzer, bei Streams und in OTTO. Gemeinsam mit dem Standard-Teachable-Machine-Modell landen die Werte in `app-settings.json`, damit Analyzer, Batch-/Stream-Endpunkte und der OTTO-Chat dieselbe Provider-Konfiguration verwenden und nach Neustarts synchron bleiben.

## WiFi Broadcast (ottcolab.local)
1. Installiere die Requirements (wir shippen `zeroconf`, wichtig fÃ¼r mDNS). Falls du ein bestehendes Environment nutzt, fÃ¼hre `pip install zeroconf` aus.
2. Starte `uvicorn app:app --host 0.0.0.0 --port 8000`, sodass der Server im WLAN erreichbar ist.
3. Ã–ffne `http://localhost:8000/config`, scrolle zum Abschnitt â€WiFi Broadcast & ottcolab.localâ€œ.
4. Hostname setzen (wir erzwingen `.local`) und den Port bestÃ¤tigen, anschlieÃŸend â€Broadcast aktivierenâ€œ anklicken.
5. Jetzt sollten Smartphones, Tablets und Desktop-GerÃ¤te im selben Netzwerk `http://ottcolab.local:8000/` aufrufen kÃ¶nnen. Feedback bitte weiterhin an **otcdmin@outlook.com**, Instagram **@ottcouture.eu** oder [Discord](https://discord.gg/GMMSqePfPh).

## Teachable Machine Depot (`/TM-models`)
1. Exportiere dein Google Teachable-Machine-Projekt als **TensorFlow** Paket (enthÃ¤lt `metadata.json`, `model.json`, `weights.bin`) oder als **Keras (.h5) Paket** mit `keras_model.h5` und `labels.txt`.
2. Ã–ffne `http://localhost:8000/config` und nutze den Abschnitt â€OTTCOUTURE Teachable Machine Depotâ€œ.
3. Nach dem Upload landet das Modell unter `/TM-models/<slug>` und wird in `TM-models/registry.json` gefÃ¼hrt.
4. Der Server wandelt TFJS-Exporte automatisch in ein TensorFlow SavedModel um (`tensorflowjs` wird hierzu clientseitig mitgeliefert). Fehlende Konverter oder defekte Bundles fÃ¼hren zu einer klaren Fehlermeldung.
5. Die Listenansicht erlaubt pro Modell den Status â€Standard im Analyzerâ€œ. Der Standard wird zusÃ¤tzlich in `app-settings.json` notiert.
6. Wird kein Community-Modell ausgewÃ¤hlt, greift der Analyzer auf `TEACHABLE_MODEL_PATH` (OPENCORE Referenz) zurÃ¼ck.

> Pflichtdateien: entweder `metadata.json`, `model.json`, `weights.bin` **oder** `keras_model.h5` plus `labels.txt`. Fehlen Bestandteile, lehnt der Upload ab.

## API Routen
- `GET /` â€“ Analyzer Landing Page mit Modellauswahl
- `GET /config` â€“ Self-Host Konfigurator & TM-Depot
- `GET /completions` â€“ OTTO Grow Chat UI
- `POST /analyze` â€“ Bild + Prompt + optional `model_id` + `analysis_mode`
- `POST /api/opencore/analyze-ml` â€“ Alias fÃ¼r ML-only Calls (identisch zu `/analyze` mit `analysis_mode=ml`)
- `POST /api/opencore/analyze-batch` â€“ Multi-Bild-Analyse (FormData mit `files[]`)
- `POST /api/opencore/share` & `GET /api/opencore/share/{id}` â€“ JSON-Share-Service (`/share/{id}` liefert Viewer)
- `POST /api/completions` â€“ OTTO Chat Endpoint (`prompt` im JSON-Body)
- `GET/POST/DELETE /api/opencore/streams*` â€“ Verwaltung der Snapshot/Video-Streams inkl. Trigger-Endpoint
- `GET /tm-models` â€“ Registry + Defaultinformationen
- `POST /tm-models/upload` â€“ ZIP Upload (`file`, `model_type`, `display_name`)
- `POST /tm-models/default/{model_id}` â€“ setzt Standardmodell
- `DELETE /tm-models/default` â€“ entfernt Standardmodell
- `GET/POST/DELETE /api/settings/llm` â€“ persistiert Provider/Prompt-Konfigurationen im Backend
- `GET /network/status`, `POST /network/announce`, `DELETE /network/announce` â€“ mDNS Steuerung

## Dokumentation im `/doc`-Verzeichnis

Alle geforderten Feature-Guides liegen als statische HTML-Seiten vor und werden Ã¼ber FastAPI unter `/doc` ausgeliefert:

- `doc/prompts.html` â€“ Vorlagen & Custom-Presets
- `doc/batch.html` â€“ Batch-Analyse mit API-Beispielen
- `doc/debug.html` â€“ Debug-Panel
- `doc/api_token_mode.html` â€“ Professional Mode
- `doc/ui.html` â€“ UI-Erweiterungen (Drag&Drop, Theme, Zoom, JSON-Fullscreen)
- `doc/export.html` â€“ JSON/PDF/Share-Export
- `doc/home_automation.html` â€“ Home-Automation Guide inkl. curl, Python, Node-RED, Home Assistant
- `doc/streams.html` â€“ Video- & Snapshot-Streams inkl. API-Aufrufen
- `doc/models.html` â€“ Teachable-Machine (Easy) und Label-Studio/YOLO (Pro) Workflows
- `doc/raspberry.html` â€“ Raspberry-Pi-Montage, Kamera-Setup und Edge-Scripting

## Projektstruktur
```
.
â”œâ”€â”€ app.py                # FastAPI Service + TM Depot + WiFi Broadcast + OTTO Endpoint
â”œâ”€â”€ static/
â”‚   â”œâ”€â”€ index.html        # Analyzer UI inkl. Modellauswahl
â”‚   â”œâ”€â”€ completions.html  # OTTO Grow Chat OberflÃ¤che
â”‚   â””â”€â”€ config.html       # Self-Host + TM Depot OberflÃ¤che
â”œâ”€â”€ TM-models/            # Versionierte Teachable-Machine Bundles (ZIP-Uploads)
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ registry.json     # wird zur Laufzeit gepflegt
â”œâ”€â”€ app-settings.json     # Standardmodell (wird bei Bedarf erzeugt)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```

## Feedback & Rechte
- Brand & Rechte: **ottcouture.eu** â€“ wir verÃ¶ffentlichen hier bewusst OpenCore, aber behalten sÃ¤mtliche Markenrechte.
- Feedback: **otcdmin@outlook.com**, Instagram **@ottcouture.eu**, Discord [`discord.gg/GMMSqePfPh`](https://discord.gg/GMMSqePfPh).
- Lizenz: [AGPL-3.0](LICENSE). Bitte alle Forks/Deployments wieder zur Community spiegeln und Credits lassen.
